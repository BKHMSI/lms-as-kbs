{
    "access": [
        {
            "title": "Language Models as Knowledge Bases?",
            "url": "https://arxiv.org/abs/1909.01066"
        },
        {
            "title": "Can Language Models be Biomedical Knowledge Bases?",
            "url": "https://arxiv.org/abs/2109.07154"
        },
        {
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "url": "https://arxiv.org/abs/2104.07885"
        },
        {
            "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",
            "url": "https://arxiv.org/abs/2001.07676"
        },
        {
            "title": "Itâ€™s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
            "url": "https://arxiv.org/abs/2009.07118"
        },
        {
            "title": "Entailment as Few-Shot Learner",
            "url": "https://arxiv.org/abs/2104.14690"
        },
        {
            "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
            "url": "https://arxiv.org/abs/2104.04670"
        },
        {
            "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
            "url": "https://arxiv.org/abs/2101.00190"
        },
        {
            "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall",
            "url": "https://arxiv.org/abs/2104.05240"
        },
        {
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "url": "https://arxiv.org/abs/2104.08691"
        },
        {
            "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
            "url": "https://arxiv.org/abs/2104.06599"
        }
    ],

    "consistency": [
        {
            "title": "Measuring and Improving Consistency in Pretrained Language Models",
            "url": "https://arxiv.org/abs/2102.01017"
        },
        {
            "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
            "url": "https://aclanthology.org/2020.acl-main.698/"
        },
        {
            "title": "Knowledge Based Multilingual Language Model",
            "url": "https://arxiv.org/abs/2111.10962"
        }
    ],

    "edit": [
        {
            "title": "Modifying Memories in Transformer Models",
            "url": "https://arxiv.org/abs/2012.00363"
        },
        {
            "title": "Knowledge Neurons in Pretrained Transformers",
            "url": "https://arxiv.org/abs/2104.08696"
        },
        {
            "title": "Editing Factual Knowledge in Language Models",
            "url": "https://arxiv.org/abs/2104.08164"
        },
        {
            "title": "Fast Model Editing at Scale",
            "url": "https://arxiv.org/abs/2110.11309"
        },
        {
            "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs",
            "url": "https://arxiv.org/abs/2111.13654"
        },
        {
            "title": "Towards Continual Knowledge Learning of Language Models",
            "url": "https://arxiv.org/abs/2110.03215"
        }
    ],

    "reasoning": [
        {
            "title": "Are Pretrained Language Models Symbolic Reasoners Over Knowledge?",
            "url": "https://aclanthology.org/2020.conll-1.45"
        },
        {
            "title": "Transformers as Soft Reasoners over Language",
            "url": "https://arxiv.org/abs/2002.05867"
        },
        {
            "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability",
            "url": "https://arxiv.org/abs/2112.09054"
        },
        {
            "title": "RULEBERT: Teaching Soft Rules to Pre-Trained Language Models",
            "url": "https://arxiv.org/abs/2109.13006"
        },
        {
            "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Model",
            "url": "https://arxiv.org/abs/2110.07178"
        },
        {
            "title": "Commonsense Reasoning with Implicit Knowledge in Natural Language",
            "url": "https://openreview.net/pdf?id=a4-fFL7aCi0"
        },
        {
            "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms",
            "url": "https://arxiv.org/abs/2005.00782"
        },
        {
            "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
            "url": "https://aclanthology.org/P19-1487"
        },
        {
            "title": "Formal Mathematics Statement Curriculum Learning",
            "url": "https://www.semanticscholar.org/paper/Formal-Mathematics-Statement-Curriculum-Learning-Polu-Han/916a06a6d51aa93de27aac2f3e14faed08dd6706"
        },
        {
            "title": "PROVER: Proof Generation for Interpretable Reasoning over Rules",
            "url": "https://aclanthology.org/2020.emnlp-main.9"
        },

        {
            "title": "Analysing Mathematical Reasoning Abilities of Neural Models",
            "url": "https://arxiv.org/abs/1904.01557"
        },
        {
            "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
            "url": "https://aclanthology.org/2021.findings-acl.317"
        },
        {
            "title": "Measuring Systematic Generalization in Neural Proof Generation with Transformers",
            "url": "https://arxiv.org/abs/2009.14786"
        },
        {
            "title": "TBD",
            "url": "#"
        }
    ],
    "interpretability": [
        {
            "title": "Interpretability and Analysis in Neural NLP",
            "url": "https://aclanthology.org/2020.acl-tutorials.1/"
        },
        {
	        "title": "Designing and Interpreting Probes with Control Tasks",
	        "url": "https://arxiv.org/abs/1909.03368"
        },
        {
            "title": "A Structural Probe for Finding Syntax in Word Representations",
            "url": "https://aclanthology.org/N19-1419/"
        },
        {
            "title": "What Do You Learn from Context? Probing for Sentence Structure in Contextualized Word Representations",
            "url": "https://arxiv.org/abs/1905.06316"
        },
        {
            "title": "Attention is not Explanation",
            "url": "https://arxiv.org/abs/1902.10186"
        },
        {
            "title": "Is Attention Interpretable?",
            "url": "https://arxiv.org/abs/1906.03731"
        },
        {
            "title": "Attention is not not Explanation",
            "url": "https://arxiv.org/abs/1908.04626"
        },
        {
            "title": "Towards Transparent and Explainable Attention Models",
            "url": "https://arxiv.org/abs/2004.14243"
        },
        {
            "title": "Of Non-Linearity and Commutativity in BERT",
            "url": "https://arxiv.org/abs/2101.04547"
        },
        {
            "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
            "url": "https://arxiv.org/abs/2012.14913"
        },
        {
            "title": "Locating and Editing Factual Knowledge in GPT",
            "url": "https://arxiv.org/abs/2202.05262"
        },
        {
            "title": "A Mathematical Framework for Transformer Circuits",
            "url": "https://transformer-circuits.pub/2021/framework/index.html"
        }
    ],
    "explainability": [
        {
            "title": "Understanding Black-box Predictions via Influence Functions",
            "url": "https://arxiv.org/abs/1703.04730"
        },
        {
            "title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions",
            "url": "https://arxiv.org/abs/2005.06676"
        },
        {
            "title": "An Empirical Comparison of Instance Attribution Methods for NLP",
            "url": "https://arxiv.org/abs/2104.04128"
        },
        {
            "title": "Combining Feature and Instance Attribution to Detect Artifacts",
            "url": "https://arxiv.org/abs/2107.00323"
        },
        {
            "title": "HILDIF: Interactive Debugging of NLI Models Using Influence Functions",
            "url": "https://aclanthology.org/2021.internlp-1.1/"
        },
        {
            "title": "WT5?! Training Text-to-Text Models to Explain their Predictions",
            "url": "https://arxiv.org/abs/2004.14546"
        },
        {
            "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations",
            "url": "https://arxiv.org/abs/1910.03065"
        },
        {
            "title": "Interactively Generating Explanations for Transformer Language Models",
            "url": "https://arxiv.org/abs/2110.02058"
        },
        {
            "title": "Rationalizing Neural Predictions",
            "url": "https://arxiv.org/abs/1606.04155"
        },
        {
            "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
            "url": "https://aclanthology.org/2021.findings-acl.366/"
        }
    ]
}
