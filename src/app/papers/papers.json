{
    "access": [
        {
            "title": "Language Models as Knowledge Bases?",
            "url": "https://arxiv.org/abs/1909.01066"
        },
        {
            "title": "Can Language Models be Biomedical Knowledge Bases?",
            "url": "https://arxiv.org/abs/2109.07154"
        },
        {
            "title": "Probing Across Time: What Does RoBERTa Know and When?",
            "url": "https://arxiv.org/abs/2104.07885"
        },
        {
            "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",
            "url": "https://arxiv.org/abs/2001.07676"
        },
        {
            "title": "Itâ€™s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
            "url": "https://arxiv.org/abs/2009.07118"
        },
        {
            "title": "Entailment as Few-Shot Learner",
            "url": "https://arxiv.org/abs/2104.14690"
        },
        {
            "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections",
            "url": "https://arxiv.org/abs/2104.04670"
        },
        {
            "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
            "url": "https://arxiv.org/abs/2101.00190"
        },
        {
            "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall",
            "url": "https://arxiv.org/abs/2104.05240"
        },
        {
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "url": "https://arxiv.org/abs/2104.08691"
        },
        {
            "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
            "url": "https://arxiv.org/abs/2104.06599"
        }
    ],

    "consistency": [
        {
            "title": "Measuring and Improving Consistency in Pretrained Language Models",
            "url": "https://arxiv.org/abs/2102.01017"
        },
        {
            "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",
            "url": "https://aclanthology.org/2020.acl-main.698/"
        },
        {
            "title": "Knowledge Based Multilingual Language Model",
            "url": "https://arxiv.org/abs/2111.10962"
        }
    ],

    "edit": [
        {
            "title": "Modifying Memories in Transformer Models",
            "url": "https://arxiv.org/abs/2012.00363"
        },
        {
            "title": "Knowledge Neurons in Pretrained Transformers",
            "url": "https://arxiv.org/abs/2104.08696"
        },
        {
            "title": "Editing Factual Knowledge in Language Models",
            "url": "https://arxiv.org/abs/2104.08164"
        },
        {
            "title": "Fast Model Editing at Scale",
            "url": "https://arxiv.org/abs/2110.11309"
        },
        {
            "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs",
            "url": "https://arxiv.org/abs/2111.13654"
        },
        {
            "title": "Towards Continual Knowledge Learning of Language Models",
            "url": "https://arxiv.org/abs/2110.03215"
        }
    ],

    "reasoning": [
        {
            "title": "Are Pretrained Language Models Symbolic Reasoners Over Knowledge?",
            "url": "https://aclanthology.org/2020.conll-1.45"
        },
        {
            "title": "Transformers as Soft Reasoners over Language",
            "url": "https://arxiv.org/abs/2002.05867"
        },
        {
            "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability",
            "url": "https://arxiv.org/abs/2112.09054"
        },
        {
            "title": "RULEBERT: Teaching Soft Rules to Pre-Trained Language Models",
            "url": "https://arxiv.org/abs/2109.13006"
        },
        {
            "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Model",
            "url": "https://arxiv.org/abs/2110.07178"
        },
        {
            "title": "Commonsense Reasoning with Implicit Knowledge in Natural Language",
            "url": "https://openreview.net/pdf?id=a4-fFL7aCi0"
        },
        {
            "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms",
            "url": "https://arxiv.org/abs/2005.00782"
        },
        {
            "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
            "url": "https://aclanthology.org/P19-1487"
        },
        {
            "title": "Formal Mathematics Statement Curriculum Learning",
            "url": "https://www.semanticscholar.org/paper/Formal-Mathematics-Statement-Curriculum-Learning-Polu-Han/916a06a6d51aa93de27aac2f3e14faed08dd6706"
        },
        {
            "title": "PROVER: Proof Generation for Interpretable Reasoning over Rules",
            "url": "https://aclanthology.org/2020.emnlp-main.9"
        },

        {
            "title": "Analysing Mathematical Reasoning Abilities of Neural Models",
            "url": "https://arxiv.org/abs/1904.01557"
        },
        {
            "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
            "url": "https://aclanthology.org/2021.findings-acl.317"
        },
        {
            "title": "Measuring Systematic Generalization in Neural Proof Generation with Transformers",
            "url": "https://arxiv.org/abs/2009.14786"
        },
        {
            "title": "TBD",
            "url": "#"
        }
    ],
    "interpretability": [
        {
            "title": "Interpretability and Analysis in Neural NLP",
            "url": "https://aclanthology.org/2020.acl-tutorials.1/"
        },
        {
	        "title": "Designing and Interpreting Probes with Control Tasks",
	        "url": "https://arxiv.org/abs/1909.03368"
        },
        {
            "title": "A Structural Probe for Finding Syntax in Word Representations",
            "url": "https://aclanthology.org/N19-1419/"
        },
        {
            "title": "What Do You Learn from Context? Probing for Sentence Structure in Contextualized Word Representations",
            "url": "https://arxiv.org/abs/1905.06316"
        },
        {
            "title": "Attention is not Explanation",
            "url": "https://arxiv.org/abs/1902.10186"
        },
        {
            "title": "Is Attention Interpretable?",
            "url": "https://arxiv.org/abs/1906.03731"
        },
        {
            "title": "Attention is not not Explanation",
            "url": "https://arxiv.org/abs/1908.04626"
        },
        {
            "title": "Towards Transparent and Explainable Attention Models",
            "url": "https://arxiv.org/abs/2004.14243"
        },
        {
            "title": "Of Non-Linearity and Commutativity in BERT",
            "url": "https://arxiv.org/abs/2101.04547"
        },
        {
            "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
            "url": "https://arxiv.org/abs/2012.14913"
        },
        {
            "title": "Locating and Editing Factual Knowledge in GPT",
            "url": "https://arxiv.org/abs/2202.05262"
        },
        {
            "title": "A Mathematical Framework for Transformer Circuits",
            "url": "https://transformer-circuits.pub/2021/framework/index.html"
        }
    ],
    "explainability": [
        {
            "title": "Understanding Black-box Predictions via Influence Functions",
            "url": "https://arxiv.org/abs/1703.04730"
        },
        {
            "title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions",
            "url": "https://arxiv.org/abs/2005.06676"
        },
        {
            "title": "An Empirical Comparison of Instance Attribution Methods for NLP",
            "url": "https://arxiv.org/abs/2104.04128"
        },
        {
            "title": "Combining Feature and Instance Attribution to Detect Artifacts",
            "url": "https://arxiv.org/abs/2107.00323"
        },
        {
            "title": "HILDIF: Interactive Debugging of NLI Models Using Influence Functions",
            "url": "https://aclanthology.org/2021.internlp-1.1/"
        },
        {
            "title": "WT5?! Training Text-to-Text Models to Explain their Predictions",
            "url": "https://arxiv.org/abs/2004.14546"
        },
        {
            "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations",
            "url": "https://arxiv.org/abs/1910.03065"
        },
        {
            "title": "Interactively Generating Explanations for Transformer Language Models",
            "url": "https://arxiv.org/abs/2110.02058"
        },
        {
            "title": "Rationalizing Neural Predictions",
            "url": "https://arxiv.org/abs/1606.04155"
        },
        {
            "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
            "url": "https://aclanthology.org/2021.findings-acl.366/"
        }
    ],
    "models": [
        {
            "title": "A Neural Knowledge Language Model",
            "url": "https://arxiv.org/abs/1608.00318"
        },
        {
            "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction",
            "url": "https://arxiv.org/abs/1906.05317"
        },
        {
            "title": "Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling",
            "url": "https://arxiv.org/abs/1906.07241"
        },
        {
            "title": "Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph",
            "url": "https://arxiv.org/abs/2009.11692"
        },
        {
            "title": "BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models",
            "url": "https://aclanthology.org/2020.findings-emnlp.207/"
        },
        {
            "title": "Knowledge Enhanced Contextual Word Representations",
            "url": "https://arxiv.org/abs/1909.04164"
        },
        {
            "title": "Entities as Experts: Sparse Memory Access with Entity Supervision",
            "url": "https://arxiv.org/abs/2004.07202"
        },
        {
            "title": "Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge",
            "url": "https://arxiv.org/abs/2007.00849"
        },
        {
            "title": "Reasoning Over Virtual Knowledge Bases with Open Predicate Relations",
            "url": "https://arxiv.org/abs/2102.07043"
        },
        {
            "title": "ERNIE: Enhanced Language Representation with Informative Entities",
            "url": "https://arxiv.org/abs/1905.07129"
        },
        {
            "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
            "url": "https://arxiv.org/abs/1911.06136"
        },
        {
            "title": "Knowledge-Aware Language Model Pretraining",
            "url": "https://arxiv.org/abs/2007.00655"
        },
        {
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "url": "https://arxiv.org/abs/2005.11401"
        },
        {
            "title": "Differentiable Reasoning over a Virtual Knowledge Base",
            "url": "https://arxiv.org/abs/2002.10640"
        },
        {
            "title": "Mention Memory: Incorporating Textual Knowledge Into Transformers Through Entity Mention Attention",
            "url": "https://arxiv.org/abs/2110.06176"
        },
        {
            "title": "Adaptive Semiparametric Language Models",
            "url": "https://arxiv.org/abs/2102.02557"
        },
        {
            "title": "Episodic Memory in Lifelong Language Learning",
            "url": "https://arxiv.org/abs/1906.01076"
        },
        {
            "title": "Relational Memory Augmented Language Models",
            "url": "https://arxiv.org/abs/2201.09680"
        },
        {
            "title": "K-BERT: Enabling Language Representation with Knowledge Graph",
            "url": "https://arxiv.org/abs/1909.07606"
        },
        {
            "title": "LUKE: Deep Contextualized Entity Representations with Entity-Aware Self-Attention",
            "url": "https://aclanthology.org/2020.emnlp-main.523/"
        },
        {
            "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
            "url": "https://arxiv.org/abs/2110.08151"
        },
        {
            "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning",
            "url": "https://arxiv.org/abs/2012.15022"
        },
        {
            "title": "KG-BERT: BERT for Knowledge Graph Completion",
            "url": "https://arxiv.org/abs/1909.03193"
        },
        {
            "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
            "url": "https://arxiv.org/abs/2107.06955"
        },
        {
            "title": "CM3: A Causal Masked Multimodal Model of the Internet",
            "url": "https://arxiv.org/abs/2201.07520"
        },
        {
            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
            "url": "https://arxiv.org/abs/2002.08909"
        },
        {
            "title": "Multi-task Retrieval for Knowledge-Intensive Tasks",
            "url": "https://arxiv.org/abs/2101.00117"
        }
    ]
}
